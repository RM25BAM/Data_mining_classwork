options(repos = c(CRAN = "http://cran.rstudio.com"))
install.packages("rpart")
install.packages("rpart.plot")
install.packages("caret")
library(rpart)
library(rpart.plot)
library(caret)
# Set seed for reproducibility
set.seed(1234)
# Load dataset
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
adult_data <- read.csv(url, header = FALSE)
# Assign column names
col_names <- c("age", "workclass", "fnlwgt", "education", "education-num", "marital-status",
"occupation", "relationship", "race", "sex", "capital-gain", "capital-loss",
"hours-per-week", "native-country", "income")
colnames(adult_data) <- col_names
# Remove any rows with missing values
adult_data <- na.omit(adult_data)
# Convert income to a factor
adult_data$income <- as.factor(adult_data$income)
# Split dataset into training and validation sets (70-30 split)
train_index <- createDataPartition(adult_data$income, p = 0.7, list = FALSE)
train_data <- adult_data[train_index, ]
valid_data <- adult_data[-train_index, ]
# Train decision tree classifier
tree_model <- rpart(income ~ ., data = train_data, method = "class")
# Display the cp table
printcp(tree_model)
# Choose cp value based on cp table
cp_value <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]),"CP"]
# Prune the tree with the chosen cp value
pruned_tree_model <- prune(tree_model, cp = cp_value)
# Plot pruned decision tree
rpart.plot(pruned_tree_model, box.palette = "RdBu", shadow.col = "gray", main = "Pruned Decision Tree")
# Predict on validation set
pred <- predict(pruned_tree_model, valid_data, type = "class")
# Evaluate the model
confusion_matrix <- confusionMatrix(pred, valid_data$income)
print(confusion_matrix)
#test\
#options(repos = c(CRAN = "http://cran.rstudio.com"))
#install.packages("rpart")
#install.packages("rpart.plot")
#install.packages("caret")
# Load required libraries
library(rpart)
library(rpart)
library(rpart.plot)
library(caret)
library(ggplot2)
library(lattice)
# Set seed for reproducibility
set.seed(1234)
# Load dataset
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
adult_data <- read.csv(url, header = FALSE)
# Assign column names
col_names <- c("age", "workclass", "fnlwgt", "education", "education-num", "marital-status",
"occupation", "relationship", "race", "sex", "capital-gain", "capital-loss",
"hours-per-week", "native-country", "income")
colnames(adult_data) <- col_names
# Remove any rows with missing values
adult_data <- na.omit(adult_data)
# Convert income to a factor
adult_data$income <- as.factor(adult_data$income)
# Split dataset into training and validation sets (70-30 split)
train_index <- createDataPartition(adult_data$income, p = 0.7, list = FALSE)
train_data <- adult_data[train_index, ]
valid_data <- adult_data[-train_index, ]
# Train decision tree classifier
tree_model <- rpart(income ~ ., data = train_data, method = "class")
# Display the cp table
printcp(tree_model)
# Choose cp value based on cp table
cp_value <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]),"CP"]
# Prune the tree with the chosen cp value
pruned_tree_model <- prune(tree_model, cp = cp_value)
# Plot pruned decision tree
rpart.plot(pruned_tree_model, box.palette = "RdBu", shadow.col = "gray", main = "Pruned Decision Tree")
# Predict on validation set
pred <- predict(pruned_tree_model, valid_data, type = "class")
# Evaluate the model
confusion_matrix <- confusionMatrix(pred, valid_data$income)
print(confusion_matrix)
library(rpart)
library(rpart.plot)
# Load dataset
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
adult <- read.table(url, sep = ",", header = FALSE, na.strings = "?")
names(adult) <- c("age", "workclass", "fnlwgt", "education", "education_num", "marital_status",
"occupation", "relationship", "race", "sex", "capital_gain", "capital_loss",
"hours_per_week", "native_country", "income")
# Preprocess data
df <- adult[-3] # Remove 'fnlwgt' column
adult$income <- factor(adult$income, levels = c(" <=50K", " >50K"), labels = c("<=50K", ">50K"))
# Split data into train and validation sets
set.seed(1234)
train <- sample(nrow(df), 0.7 * nrow(df))
df.train <- df[train, ]
df.validate <- df[-train, ]
# Train decision tree classifier
dtree <- rpart(income ~ ., data = df.train, method = "class", parms = list(split = "information"))
# Display and choose cp value
print("CP Table:")
printcp(dtree)
cp_value <- dtree$cptable[which.min(dtree$cptable[,"xerror"]), "CP"]
# Prune the tree
dtree.pruned <- prune(dtree, cp = cp_value)
# Plot pruned decision tree with color
prp(dtree.pruned, type = 2, extra = 104, main = "Pruned Decision Tree for Adult Dataset",
fallen.leaves = TRUE, box.col = ifelse(dtree.pruned$frame$yval == "<=50K", "lightblue", "lightgreen"))
library(rpart)
library(rpart.plot)
# Load dataset
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
adult <- read.table(url, sep = ",", header = FALSE, na.strings = "?")
names(adult) <- c("age", "workclass", "fnlwgt", "education", "education_num", "marital_status",
"occupation", "relationship", "race", "sex", "capital_gain", "capital_loss",
"hours_per_week", "native_country", "income")
# Preprocess data
df <- adult[-3] # Remove 'fnlwgt' column
adult$income <- factor(adult$income, levels = c(" <=50K", " >50K"), labels = c("<=50K", ">50K"))
# Split data into train and validation sets
set.seed(1234)
train <- sample(nrow(df), 0.7 * nrow(df))
df.train <- df[train, ]
df.validate <- df[-train, ]
# Train decision tree classifier
dtree <- rpart(income ~ ., data = df.train, method = "class", parms = list(split = "information"))
# Display and choose cp value
print("CP Table:")
printcp(dtree)
cp_value <- dtree$cptable[which.min(dtree$cptable[,"xerror"]), "CP"]
# Prune the tree
dtree.pruned <- prune(dtree, cp = cp_value)
# Plot pruned decision tree with color
prp(dtree.pruned, type = 2, extra = 104, main = "Pruned Decision Tree for Adult Dataset",
fallen.leaves = TRUE, box.col = ifelse(dtree.pruned$frame$yval == "<=50K", "lightblue", "lightgreen"))
# Predict on validation set
dtree.pred <- predict(dtree.pruned, newdata = df.validate, type = "class")
# Evaluate the model
dtree.perf <- table(actual = df.validate$income, predicted = dtree.pred, dnn = c("Actual", "Predicted"))
print("Confusion Matrix:")
print(dtree.perf)
# Compute evaluation metrics
tn <- dtree.perf[1, 1]
fp <- dtree.perf[1, 2]
fn <- dtree.perf[2, 1]
tp <- dtree.perf[2, 2]
accuracy <- (tp + tn) / (tp + tn + fp + fn)
error.rate <- (fp + fn) / (tp + tn + fp + fn)
sensitivity <- tp / (tp + fp)
specificity <- tn / (tn + fp)
precision <- tp / (tp + fp)
recall <- tp / (tp + fn)
f.measure <- (2 * precision * recall) / (precision + recall)
# Print evaluation metrics
print(paste("Accuracy:", accuracy))
print(paste("Error Rate:", error.rate))
print(paste("Sensitivity:", sensitivity))
print(paste("Specificity:", specificity))
print(paste("Precision:", precision))
print(paste("Recall:", recall))
print(paste("F-measure:", f.measure))
library(rpart)
library(rpart.plot)
# Load dataset
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
adult <- read.table(url, sep = ",", header = FALSE, na.strings = "?")
names(adult) <- c("age", "workclass", "fnlwgt", "education", "education_num", "marital_status",
"occupation", "relationship", "race", "sex", "capital_gain", "capital_loss",
"hours_per_week", "native_country", "income")
df <- adult[-3] # 'fnlwgt' not needed
adult$income <- factor(adult$income, levels = c(" <=50K", " >50K"), labels = c("<=50K", ">50K"))
set.seed(1234)
train <- sample(nrow(df), 0.7 * nrow(df))
df.train <- df[train, ]
df.validate <- df[-train, ]
dtree <- rpart(income ~ ., data = df.train, method = "class", parms = list(split = "information"))
print("CP Table:")
printcp(dtree)
cp_value <- dtree$cptable[which.min(dtree$cptable[,"xerror"]), "CP"]
dtree.pruned <- prune(dtree, cp = cp_value)
prp(dtree.pruned, type = 2, extra = 104, main = "Pruned Decision Tree Adult Dataset",
fallen.leaves = TRUE, box.col = ifelse(dtree.pruned$frame$yval == "<=50K", "lightblue", "lightred"))
library(rpart)
library(rpart.plot)
# Load dataset
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
adult <- read.table(url, sep = ",", header = FALSE, na.strings = "?")
names(adult) <- c("age", "workclass", "fnlwgt", "education", "education_num", "marital_status",
"occupation", "relationship", "race", "sex", "capital_gain", "capital_loss",
"hours_per_week", "native_country", "income")
df <- adult[-3] # 'fnlwgt' not needed
adult$income <- factor(adult$income, levels = c(" <=50K", " >50K"), labels = c("<=50K", ">50K"))
set.seed(1234)
train <- sample(nrow(df), 0.7 * nrow(df))
df.train <- df[train, ]
df.validate <- df[-train, ]
dtree <- rpart(income ~ ., data = df.train, method = "class", parms = list(split = "information"))
print("CP Table:")
printcp(dtree)
cp_value <- dtree$cptable[which.min(dtree$cptable[,"xerror"]), "CP"]
dtree.pruned <- prune(dtree, cp = cp_value)
prp(dtree.pruned, type = 2, extra = 104, main = "Pruned Decision Tree Adult Dataset",
fallen.leaves = TRUE, box.col = ifelse(dtree.pruned$frame$yval == "<=50K", "lightblue", "red"))
dtree.pred <- predict(dtree.pruned, newdata = df.validate, type = "class")
dtree.perf <- table(actual = df.validate$income, predicted = dtree.pred, dnn = c("Actual", "Predicted"))
print("Confusion Matrix:")
print(dtree.perf)
tn <- dtree.perf[1, 1]
fp <- dtree.perf[1, 2]
fn <- dtree.perf[2, 1]
tp <- dtree.perf[2, 2]
accuracy <- (tp + tn) / (tp + tn + fp + fn)
error.rate <- (fp + fn) / (tp + tn + fp + fn)
sensitivity <- tp / (tp + fp)
specificity <- tn / (tn + fp)
precision <- tp / (tp + fp)
recall <- tp / (tp + fn)
f.measure <- (2 * precision * recall) / (precision + recall)
# Print evaluation metrics
print(paste("Accuracy:", accuracy))
print(paste("Error Rate:", error.rate))
print(paste("Sensitivity:", sensitivity))
print(paste("Specificity:", specificity))
print(paste("Precision:", precision))
print(paste("Recall:", recall))
print(paste("F-measure:", f.measure))
library(rpart)
library(rpart.plot)
# Load dataset
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
adult <- read.table(url, sep = ",", header = FALSE, na.strings = "?")
names(adult) <- c("age", "workclass", "fnlwgt", "education", "education_num", "marital_status",
"occupation", "relationship", "race", "sex", "capital_gain", "capital_loss",
"hours_per_week", "native_country", "income")
df <- adult[-3] # 'fnlwgt' not needed
adult$income <- factor(adult$income, levels = c(" <=50K", " >50K"), labels = c("<=50K", ">50K"))
set.seed(1234)
train <- sample(nrow(df), 0.7 * nrow(df))
df.train <- df[train, ]
df.validate <- df[-train, ]
dtree <- rpart(income ~ ., data = df.train, method = "class", parms = list(split = "information"))
print("CP Table:")
printcp(dtree)
cp_value <- dtree$cptable[which.min(dtree$cptable[,"xerror"]), "CP"]
dtree.pruned <- prune(dtree, cp = cp_value)
prp(dtree.pruned, type = 2, extra = 104, main = "Pruned Decision Tree Adult Dataset",
fallen.leaves = TRUE, box.col = ifelse(dtree.pruned$frame$yval == "<=50K", "lightblue", "pink"))
dtree.pred <- predict(dtree.pruned, newdata = df.validate, type = "class")
dtree.perf <- table(actual = df.validate$income, predicted = dtree.pred, dnn = c("Actual", "Predicted"))
print("Confusion Matrix:")
print(dtree.perf)
tn <- dtree.perf[1, 1]
fp <- dtree.perf[1, 2]
fn <- dtree.perf[2, 1]
tp <- dtree.perf[2, 2]
accuracy <- (tp + tn) / (tp + tn + fp + fn)
error.rate <- (fp + fn) / (tp + tn + fp + fn)
sensitivity <- tp / (tp + fp)
specificity <- tn / (tn + fp)
precision <- tp / (tp + fp)
recall <- tp / (tp + fn)
f.measure <- (2 * precision * recall) / (precision + recall)
# Print evaluation metrics
print(paste("Accuracy:", accuracy))
print(paste("Error Rate:", error.rate))
print(paste("Sensitivity:", sensitivity))
print(paste("Specificity:", specificity))
print(paste("Precision:", precision))
print(paste("Recall:", recall))
print(paste("F-measure:", f.measure))
library(rpart)
library(rpart.plot)
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
adult_data <- read.table(url, sep = ",", header = FALSE, na.strings = "?")
names(adult_data) <- c("age", "workclass", "fnlwgt", "education", "education_num", "marital_status",
"occupation", "relationship", "race", "sex", "capital_gain", "capital_loss",
"hours_per_week", "native_country", "income")
df <- adult_data[-3] # Remove 'fnlwgt' column
adult_data$income <- factor(adult_data$income, levels = c(" <=50K", " >50K"), labels = c("<=50K", ">50K"))
set.seed(1234)
train <- sample(nrow(df), 0.7 * nrow(df))
df.train <- df[train, ]
df.validate <- df[-train, ]
dtree <- rpart(income ~ ., data = df.train, method = "class", parms = list(split = "information"))
print("CP Table:")
printcp(dtree)
cp_value <- dtree$cptable[which.min(dtree$cptable[,"xerror"]), "CP"]
dtree.pruned <- prune(dtree, cp = cp_value)
prp(dtree.pruned, type = 2, extra = 104, main = "Pruned Decision Tree Adult Dataset",
fallen.leaves = TRUE, box.col = ifelse(dtree.pruned$frame$yval == "<=50K", "lightblue", "pink"))
# Predict on validation set
dtree.pred <- predict(dtree.pruned, newdata = df.validate, type = "class")
dtree.perf <- table(actual = df.validate$income, predicted = dtree.pred, dnn = c("Actual", "Predicted"))
print("Confusion Matrix:")
print(dtree.perf)
tn <- dtree.perf[1, 1]
fp <- dtree.perf[1, 2]
fn <- dtree.perf[2, 1]
tp <- dtree.perf[2, 2]
accuracy <- (tp + tn) / (tp + tn + fp + fn)
error.rate <- (fp + fn) / (tp + tn + fp + fn)
sensitivity <- tp / (tp + fp)
specificity <- tn / (tn + fp)
precision <- tp / (tp + fp)
recall <- tp / (tp + fn)
f.measure <- (2 * precision * recall) / (precision + recall)
print(paste("Accuracy:", accuracy))
print(paste("Error Rate:", error.rate))
print(paste("Sensitivity:", sensitivity))
print(paste("Specificity:", specificity))
print(paste("Precision:", precision))
print(paste("Recall:", recall))
print(paste("F-measure:", f.measure))
library(rpart)
library(rpart.plot)
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
adult_data <- read.table(url, sep = ",", header = FALSE, na.strings = "?")
names(adult_data) <- c("age", "workclass", "fnlwgt", "education", "education_num", "marital_status",
"occupation", "relationship", "race", "sex", "capital_gain", "capital_loss",
"hours_per_week", "native_country", "income")
df <- adult_data[-3] # Remove 'fnlwgt' column
adult_data$income <- factor(adult_data$income, levels = c(" <=50K", " >50K"), labels = c("<=50K", ">50K"))
set.seed(1234)
train <- sample(nrow(df), 0.7 * nrow(df))
df.train <- df[train, ]
df.validate <- df[-train, ]
dtree <- rpart(income ~ ., data = df.train, method = "class", parms = list(split = "information"))
print("CP Table:")
printcp(dtree)
cp_value <- dtree$cptable[which.min(dtree$cptable[,"xerror"]), "CP"]
dtree.pruned <- prune(dtree, cp = cp_value)
prp(dtree.pruned, type = 2, extra = 104, main = "Pruned Decision Tree Adult Dataset",
fallen.leaves = TRUE, box.col = ifelse(dtree.pruned$frame$yval == "<=50K", "lightblue", "pink"))
# Predict on validation set
dtree.pred <- predict(dtree.pruned, newdata = df.validate, type = "class")
dtree.perf <- table(actual = df.validate$income, predicted = dtree.pred, dnn = c("Actual", "Predicted"))
print("Confusion Matrix:")
print(dtree.perf)
tn <- dtree.perf[1, 1]
fp <- dtree.perf[1, 2]
fn <- dtree.perf[2, 1]
tp <- dtree.perf[2, 2]
accuracy <- (tp + tn) / (tp + tn + fp + fn)
error.rate <- (fp + fn) / (tp + tn + fp + fn)
sensitivity <- tp / (tp + fp)
specificity <- tn / (tn + fp)
precision <- tp / (tp + fp)
recall <- tp / (tp + fn)
f.measure <- (2 * precision * recall) / (precision + recall)
print(paste("Accuracy:", accuracy))
print(paste("Error Rate:", error.rate))
print(paste("Sensitivity:", sensitivity))
print(paste("Specificity:", specificity))
print(paste("Precision:", precision))
print(paste("Recall:", recall))
print(paste("F-measure:", f.measure))
plot1 <- ggplot(churn_data, aes(x = Churn)) +
geom_bar(fill = "pink") +
labs(title = "Churn users vs non-churn users", x = "Churn", y = "Count")
plot1 <- ggplot(churn_data, aes(x = Churn)) +
geom_bar(fill = "pink") +
labs(title = "Churn users vs non-churn users", x = "Churn", y = "Count")
# Plot 1
plot1 <- ggplot(churn_data, aes(x = Churn)) +
geom_bar(fill = "pink") +
labs(title = "Churn users vs non-churn users", x = "Churn", y = "Count")
library(dplyr)
library(ggplot2)
library(ggpubr)
library(tidyr)
churn_data <- read.csv("churn.txt")
library(dplyr)
library(ggplot2)
library(ggpubr)
library(tidyr)
churn_data <- read.csv("churn.txt")
library(dplyr)
library(ggplot2)
library(ggpubr)
library(tidyr)
churn_data <- read.csv("churn.txt")
churn_data <- read.csv("churn.txt")
churn_data <- read.csv("churn.txt")
churn_data <- read.csv("churn.txt")
churn_data <- read.csv("churn.txt")
churn_data <- read.csv("churn.txt")
churn_data <- read.csv("churn.txt")
setwd("/Users/natashapiedrabuena/Desktop/Final R project")
library(dplyr)
library(ggplot2)
library(ggpubr)
library(tidyr)
churn_data <- read.csv("churn.txt")
churn_data <- as_tibble(churn_data)
#print(colnames(churn_data))
churn_data <- churn_data %>%
mutate(
State = as.factor(State),
AreaCode = as.factor(Area.Code),
InternationalPlan = as.factor(Int.l.Plan),
VoiceMailPlan = as.factor(VMail.Plan),
Churn = as.factor(Churn.)
)
# Plot 1
plot1 <- ggplot(churn_data, aes(x = Churn)) +
geom_bar(fill = "pink") +
labs(title = "Churn users vs non-churn users", x = "Churn", y = "Count")
print(plot1)
state_summary <- churn_data %>%
group_by(State) %>%
summarise(TotalCalls = sum(Day.Calls + Eve.Calls + Night.Calls),
TotalCharges = sum(Day.Charge + Eve.Charge + Night.Charge),
ChurnRate = mean(as.numeric(Churn)))
ggplot(state_summary, aes(x = TotalCalls, y = TotalCharges, size = ChurnRate, color = ChurnRate)) +
geom_point(alpha = 0.6) +
scale_size(range = c(3, 10)) +
labs(title = "Total Calls and Charges with Churn Rate", x = "Total Calls", y = "Total Charges")
#Plot 10
plot10 <- ggplot(churn_data, aes(x = Day.Mins, y = Eve.Mins, color = Churn)) +
geom_point(alpha = 0.6) +
labs(title = "Day vs. Evening Minutes Usage by Churn", x = "Day Minutes", y = "Evening Minutes")
print(plot10)
plot8 <- ggplot(churn_data, aes(x = Churn, y = Account.Length)) +
geom_violin(fill = "green") +
labs(title = "Account Length Distribution by Churn", x = "Churn", y = "Account Length")
print(plot8)
state_summary <- churn_data %>%
group_by(State) %>%
summarise(TotalCalls = sum(Day.Calls + Eve.Calls + Night.Calls),
TotalCharges = sum(Day.Charge + Eve.Charge + Night.Charge),
ChurnRate = mean(as.numeric(Churn)))
ggplot(state_summary, aes(x = TotalCalls, y = TotalCharges, size = ChurnRate, color = ChurnRate)) +
geom_point(alpha = 0.6) +
scale_size(range = c(3, 10)) +
labs(title = "Total Calls and Charges with Churn Rate", x = "Total Calls", y = "Total Charges")
churn_data <- churn_data %>%
mutate(TotalCharges = Day.Charge + Eve.Charge + Night.Charge)
plot5 <- ggplot(churn_data, aes(x = TotalCharges, fill = Churn)) +
geom_density(alpha = 0.5) +
labs(title = "Total Charges Density by Churn", x = "Total Charges", y = "Density")
print(plot5)
# Plot 1 SHOW
plot1 <- ggplot(churn_data, aes(x = Churn)) +
geom_bar(fill = "pink") +
labs(title = "Churn users vs non-churn users", x = "Churn", y = "Count")
print(plot1)
# Plot 1 SHOW
plot1 <- ggplot(churn_data, aes(x = Churn)) +
geom_bar(fill = "pink") +
labs(title = "Churn users vs non-churn users", x = "Churn", y = "Count")
print(plot1)
churn_data <- churn_data %>%
mutate(TotalCharges = Day.Charge + Eve.Charge + Night.Charge)
plot5 <- ggplot(churn_data, aes(x = TotalCharges, fill = Churn)) +
geom_density(alpha = 0.5) +
labs(title = "Total Charges Density by Churn", x = "Total Charges", y = "Density")
print(plot5)
plot7 <- ggplot(churn_data, aes(x = State, fill = Churn)) +
geom_bar() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(title = "Churn Rate by State", x = "State", y = "Count")
print(plot7)
